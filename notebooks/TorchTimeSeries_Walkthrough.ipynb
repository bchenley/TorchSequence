{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOW9U4uuZ+9QOwr1MwOujNy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bchenley/TorchTimeSeries/blob/main/notebooks/TorchTimeSeries_Walkthrough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this notebook is to walk through of how to use TorchTimeSeries for time series modeling.\n",
        "\n",
        "**Step 1: Clone the TorchTimeSeries Repository**\n",
        "\n",
        "The first step is to clone the TorchTimeSeries repository to your local machine. This will allow you to access the package and its contents. Open your command line or terminal and navigate to the directory where you want to clone the repository. Run the following command to clone the repository:"
      ],
      "metadata": {
        "id": "NELZ2zUmXOaT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKEEb0FB9ke4"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/bchenley/TorchTimeSeries.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wait for the cloning process to complete. Once it's done, you will have a local copy of the TorchTimeSeries repository on your machine.\n",
        "\n",
        "**Step 2: Install the TorchTimeSeries package and its dependencies**\n",
        "\n",
        "Now that you have cloned the repository, you need to install the TorchTimeSeries package and its dependencies. Change your current directory to the cloned repository:\n",
        "\n"
      ],
      "metadata": {
        "id": "0POMjW_dXVar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/TorchTimeSeries"
      ],
      "metadata": {
        "id": "dKCcb8bQORzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, you need to install torch and pytorch_lightning packages. Run the following commands:"
      ],
      "metadata": {
        "id": "0ex70E-mWieQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch --quiet\n",
        "print(\"torch installed.\")\n",
        "!pip install pytorch_lightning --quiet\n",
        "print(\"pytorch_lightning installed.\")"
      ],
      "metadata": {
        "id": "GSM6vroNp2Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, you can import the package and other dependencies:"
      ],
      "metadata": {
        "id": "oLfuACHxXlFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src import *"
      ],
      "metadata": {
        "id": "-vqQlwVK2G9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "S9O5ihePXypT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets consider a simple example to illustrate the use of TorchTimeSeries. First, we need data. Here, we will create synthetic data using our Laguerre Recurrent Unit (LRU) module. The LRU is an RNN based on the autorecursive relation of the discrete Laguerre basis, a set of orthogonal functions that take the form of exponentially decaying polynomials. It is characterized by a single trainable parameter (defined by the `relax` parameter) that governs the spread of the functions."
      ],
      "metadata": {
        "id": "bGIqEvfNX1Na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lru = LRU(input_size = 1, hidden_size = 3, relax_init = [0.8])\n",
        "\n",
        "b = lru.generate_laguerre_functions(120)\n",
        "\n",
        "w = torch.tensor([[1., -1., 2.]])\n",
        "\n",
        "N = 1024\n",
        "\n",
        "t = torch.arange(N)\n",
        "\n",
        "X = torch.randn((1, N, 1))\n",
        "\n",
        "with torch.no_grad():\n",
        "  V, _= lru(X)\n",
        "\n",
        "V = V.reshape(1, N, -1)\n",
        "\n",
        "h = b.squeeze(1) @ w.t()\n",
        "y2 = V @ w.t()\n",
        "\n",
        "fig, ax = plt.subplots(2, 2, figsize = (20, 5))\n",
        "\n",
        "ax[0,0].plot(b.squeeze(1), label = ['DLF 1', 'DLF 2', 'DLF 3'])\n",
        "ax[0,0].set_title(f\"relax = {lru.relax[0]: .2f}\" )\n",
        "ax[0,0].grid()\n",
        "ax[0,0].legend()\n",
        "\n",
        "ax[1,0].plot(h, label = 'h')\n",
        "ax[1,0].grid()\n",
        "ax[1,0].legend()\n",
        "\n",
        "ax[0,1].remove()\n",
        "ax[1,1].remove()\n",
        "\n",
        "ax_24 = fig.add_subplot(2, 2, (2, 4))\n",
        "ax_24.plot(t, X[0], label = 'X')\n",
        "ax_24.plot(t, y2[0], label = 'y')\n",
        "ax_24.grid()\n",
        "ax_24.legend()"
      ],
      "metadata": {
        "id": "m3vOXDbWMl3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first three discrete Laguerre functions (DLFs) are shown on the top left. The single function `h`, show on the botton left, is generated by taking a linear combination of these three functions according to the weight tensor `w`. The output `y` is generated by applying `w` to the output of the LRU `V`, where the columns of `V1`  are the convolution of the input `X` with the three DLFs. Therefore, `y` is equivalent to the convolution of the `X` with `h`."
      ],
      "metadata": {
        "id": "cDM3HCmCm3SN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our synthetic data prepared, we may prepare the data to be fed to our `DataModule`. This module employs PyTorch Lightning's `LightningDataModule`, which prepares the data and sets up the training, validation, and testing dataloaders.\n",
        "\n",
        "To do this, we place the data in a dictionary. `DataModule` expects the input-output data to be 2D, where the columns represent the features of the data and the rows represent each time index. Remember that dictionary must also contain the time variable, which may be 1D."
      ],
      "metadata": {
        "id": "H9WJm2arkImY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'X': X[0],\n",
        "        'y': y2[0],\n",
        "        't': t}"
      ],
      "metadata": {
        "id": "jmaW1fc8F6YG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We may now move on to creating our model. Here, we want to build a single-input-single-output model. This is a good place introduce the general architecture of the model. The full archicture of the model used in TorchTimeSeries is shown below, oriented left-to-right:"
      ],
      "metadata": {
        "id": "11jB-Bvttc_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "image_path = '/content/TorchTimeSeries/models/SequenceModel.drawio.png'\n",
        "\n",
        "image = Image.open(image_path)\n",
        "image.show()\n"
      ],
      "metadata": {
        "id": "l3Vk5jaouNnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is generally a multi-input-multi-output configuration with 5 possible layers.\n",
        "\n",
        "The first layer is the **sequence base layer**, which serves as a filterbank capturing the dynamics of the input-output relation. Each input `X_k` has its own individual base, which can be a Gated Recurrent Unit (GRU), Long-Short-Term Memory (LSTM) network, Laguerre Recurrent Unit (LRU), or a transformer encoder/decoder.\n",
        "\n",
        "The second layer is the **input-associated hidden layer**, in which the kth hidden unit (`HU_k`) receives the output of the sequence base layer corresponding to the kth input. This layer allows for the modeling of possible dynamic nonlinearities. The nonlinear activation functions in this layer include commonly used functions such as ReLU, sigmoid, softmax, and tanh. Additionally, a polynomial activation is provided as an alternative nonlinear activation to offer more flexibility.\n",
        "\n",
        "The third layer is the **interaction layer**, facilitating \"cross-talk\" between the inputs as they affect the outputs. Unlike the input-associated hidden layer, this layer is fully connected, so that the ith interaction unit (`IU_i`) receives the output of all input-associated hidden units. It provides the same flexibility in the choice of nonlinear activation functions.\n",
        "\n",
        "The fourth layer is the **temporal modulation layer**, responsible for modeling nonstationary behavior. Note that this layer does not directly model the time-varying characteristics of the input or output, but rather the modulation (multiplication) of the output by various functions. Currently, this layer includes options such as Legendre polynomials, Chebychev polynomials, Fourier basis functions, and the sigmoid function. However, additional functions (such as Hermite and Laguerre) can be added by the user.\n",
        "\n",
        "Finally, the fifth layer is the **output layer**, which generates the model's outputs.\n",
        "\n",
        "It's important to note that this is the full architecture, but you can specify a model with fewer layers based on your needs. In our present example, we will build a single-input-single-output architecture with an LRU base."
      ],
      "metadata": {
        "id": "TAe0LrjruudH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "model = SequenceModel(num_inputs = 1, num_outputs = 1,\n",
        "                      input_size = [1], output_size = [1],\n",
        "                      base_type = ['lru'], base_hidden_size = [3],\n",
        "                      hidden_out_features = [1], hidden_bias = [False],\n",
        "                      stateful = True)\n"
      ],
      "metadata": {
        "id": "Da-dTDEGMl3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we have specified that that we have one input and one output, each having one feature. We set the base type for each to be an LRU, both using the first three DLFs. The input-associated hidden layer is set to one feature, since we are modeling a single impulse response. Lastly, we set the model to be stateful, meaning that the hidden states generated by the LRU at a given batch will be passed to the next batch during training. We may summarize the contents of our model by printing its object:"
      ],
      "metadata": {
        "id": "JoEWNeQW4jqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G1lSNa_5nKt",
        "outputId": "6928d627-cc42-4692-979b-c1d26a625a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seq_base.0.base.relax: shape = torch.Size([1]). 1 parameters. Trainable\n",
            "hidden_layer.0.F.0.weight: shape = torch.Size([1, 3]). 3 parameters. Trainable\n",
            "output_layer.0.F.0.weight: shape = torch.Size([1, 1]). 1 parameters. Untrainable\n",
            "output_layer.0.F.0.bias: shape = torch.Size([1]). 1 parameters. Trainable\n",
            "-------------------------------------\n",
            "6 total parameters.\n",
            "5 total trainable parameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that the model is small, having only 6 parameters. Only five of them are trainable, since the inbound weight in the input-associated hidden layer is set to one due to the fact that the number of input and output features are both one, obviating the need for this weight."
      ],
      "metadata": {
        "id": "YmGV0aCv5oKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we build our datamodule, which depeonds on our custom `DataModule`:"
      ],
      "metadata": {
        "id": "CkBeOCeE527R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "datamodule = DataModule(data = data,\n",
        "                        time_name = 't', input_names = ['X'], output_names = ['y'],\n",
        "                        time_unit = 's',\n",
        "                        transforms = {'all': FeatureTransform('identity')},\n",
        "                        pct_train_val_test = [0.7, 0.15, 0.15],\n",
        "                        input_len = [1], output_len = [1],\n",
        "                        batch_size = 1,\n",
        "                        shift = [0], stride = 1)\n"
      ],
      "metadata": {
        "id": "1W5g_veR3cPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we have given the module our data dictionary and specified which variables are the input and output, along with the name of the time variable. We also tell the module not to transform (using our `FeatureTransform` class), since we know the input is already Guassian with unit variance and zero mean. We specify that we would like to use the first 70% of the entire dataset for training, the next 15% for validation, and the remaining for testing. We inform the module that the length of both the input and output is one, meaning that a given sample at time `t_i` is characterized by `{X_i, y_i}`. We use a single sample per batch, as it has been found that the LRU-based unit trains better when one time point is considered at a time."
      ],
      "metadata": {
        "id": "GmQfs_Mo6GEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we construct our Lightnight module `SequenceModule`, which perform the training and evaluation. We need to provide this module with our model, the criterion used for optimizing the model, and the optimizer used."
      ],
      "metadata": {
        "id": "tEM6LwO38IkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# lightingmodule\n",
        "loss_fn = Loss('mse', dims = (0, 1))\n",
        "\n",
        "opt = torch.optim.Adam(params = model.parameters(), betas = [0.9, 0.999], lr = 0.002)\n",
        "\n",
        "seq_module = SequenceModule(model = model,\n",
        "                            opt = opt, loss_fn = loss_fn,\n",
        "                            track = True)\n"
      ],
      "metadata": {
        "id": "DzNLyXub6FBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we specify that we will use the mean squared error (MSE) as our criterion function. The Adam optimizer with the default hyperparamters is used for optimization. We also wish the track the history of the parameters during training, so we set `track = True`."
      ],
      "metadata": {
        "id": "4LMTcznS81C3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we are ready to train the model. The SequenceModule instance has a `fit` method that is ran by providing the data prepared by our datamodule."
      ],
      "metadata": {
        "id": "TWhGhlTa9RWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [pl.callbacks.EarlyStopping(monitor = 'val_epoch_loss',\n",
        "                                        patience = 10,\n",
        "                                        verbose = False,\n",
        "                                        min_delta = 0,\n",
        "                                        mode = 'min')]\n",
        "\n",
        "seq_module.fit(datamodule = datamodule,\n",
        "               max_epochs = 20,\n",
        "               callbacks = callbacks)"
      ],
      "metadata": {
        "id": "QlMifDyD4P1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have specified the use of an `EarlyStopping` callback from PyTorch Lightning's callback library. This callback monitors the validation loss (`monitor = 'val_epoch_loss'`) and checks for improvement. If the validation loss does not improve for 5 consecutive epochs (`patience = 5`), training will stop early.\n",
        "\n",
        "The `min_delta` parameter is set to 0, indicating that any improvement in the validation loss will be considered significant. The `mode` is set to 'min' to indicate that we want to minimize the validation loss.\n",
        "\n",
        "During the training process, we pass the `datamodule` to the `fit()` function to provide the data for training. The `max_epochs` parameter is set to 500, indicating that we want to train the model for a maximum of 500 epochs.\n",
        "\n",
        "By including the `callbacks` parameter and specifying the `EarlyStopping` callback, we ensure that the training process will stop if the validation loss does not improve for a specified number of epochs, providing a way to prevent overfitting and optimize the model's performance.\n",
        "\n",
        "In this case, we've cheated, since we built a model that is based on the exact archicture used to generate the data. Therefore, we should expect perfect results."
      ],
      "metadata": {
        "id": "iK0gQJ9i96qM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the model has been trained. We can look at how well it predicts on the training, validation, and test data. To do this, we rune the `predict` method:"
      ],
      "metadata": {
        "id": "c4tk1rxheM19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_module.predict()"
      ],
      "metadata": {
        "id": "cpwJqlYMdfJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The, we can plot the predictions using the `plot_predictions` method"
      ],
      "metadata": {
        "id": "mLawI-2GfLny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_module.plot_predictions()"
      ],
      "metadata": {
        "id": "18ip9XbBdMRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot above shows the actual (black) and predicted (red) output for the training (gray region), validation (blue region) and test (red region) data. As we anticipated the prediction is perfect, so we can't see the actual signal (black trace)."
      ],
      "metadata": {
        "id": "JI7CsAbcfbc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may visualize the training history with the `plot_history` method. With `track = True` in `SequenceModule`, the history is stored in `train_history` and `val_history` (if validation data is available)."
      ],
      "metadata": {
        "id": "1HV97w3Hf_bZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(seq_module.train_history)"
      ],
      "metadata": {
        "id": "T3b4fVu_sLJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training history is collected for each step of the training process, and includes the loss (and metric if specified) and all the trainable parameters. Let's look at the loss, average over the steps in single epoch:"
      ],
      "metadata": {
        "id": "FnuJzTo3geH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_module.plot_history(['mse_y'], plot_train_history_by = 'epochs' )"
      ],
      "metadata": {
        "id": "Qh2Wltlbl2yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, in this simple example, we should expect both the training and validation loss to approach zero."
      ],
      "metadata": {
        "id": "CU9pLAjKg8ez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's look at the Laguerre paramters `relax`, which was set to 0.8 to generate the synthetic data."
      ],
      "metadata": {
        "id": "QLWUU2G8hXOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_module.plot_history(['seq_base.0.base.relax'], plot_train_history_by = 'steps')"
      ],
      "metadata": {
        "id": "NkYlIEa5sJBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results are shown over each step, instead of each epoch. Clearly the `relax` parameter converged to its actual value."
      ],
      "metadata": {
        "id": "1BnH9fQJhgPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In many cases, you might like to examine the characteristics of the model itself, as it may offer insight into the underlying system generating the input-output data. Recall that we generated our data using the output of the `LRU` model, which could be expressed as:\n",
        "\n",
        "`y = X conv h`\n",
        "\n",
        "where `h` is our \"impulse reponse function.\" Since, we know what `h` is in this exmaple, we can compare the true `h` with the model-based impulse response.\n",
        "\n",
        "We can get the model-based impulse response using the `generate_impulse_response` method. Note that to use this method, the model must have an input-associated hidden layer."
      ],
      "metadata": {
        "id": "M3xq5WW1h3Y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "impulse_response = seq_module.model.generate_impulse_response(120)\n",
        "\n",
        "plt.plot(h, 'k', label = 'Actual')\n",
        "plt.plot(impulse_response[0][0], '--r', label = 'Model-based')\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "hU0bz_FO86d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have successfully built, training, and visualized your model.\n",
        "\n",
        "Let's try and different model using the same data. Since we know that the data involves the output being some kind of filtered verion of the input, it seems intuitive that a model based on a convolution neural network (CNN) would perform well. Let's try this by replacing the LRU with a 1D CNN. To do this, we just need to change the `base_type` of our model and configure the kernel size of our CNN to match that of our impulse reponse function `h`."
      ],
      "metadata": {
        "id": "v9-MaQV-jYn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SequenceModel(num_inputs = 1, num_outputs = 1,\n",
        "                      input_size = [1], output_size = [1],\n",
        "                      base_type = ['cnn'], base_hidden_size = [1], base_cnn_bias = [False],\n",
        "                      base_cnn_kernel_size = [(120,)],\n",
        "                      hidden_out_features = [1])\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "tD9pPFd0mAkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our CNN-based model, the output value at a given time point is modeled as a linear combination of the present and past 119 input values, plus an offset at the output. This resulting in a total of 121 trainable parameters. There are 123 total parameters due the weights in input-associated hidden layer and output layer, but these untrained since the input and output of both layers have a size of 1. Note that this model is much larger than our previous model, which had only 5 parameters.\n",
        "\n",
        "We must set the `input_len` to [120] and keep the `output_len` at [1].\n",
        "\n",
        "In addition, we change our batch size from 1 to -1. This tells `DataModuel` to send all the samples at once in a single batch, however many there are.\n",
        "\n",
        "Lastly, we pad the data by setting `pad_data = True`. This ensures that all of the available data is used and is not truncated at the beginning. In this case, the training data will be padded with 119 zeros at the beginning of the input sequence. The validation data is padded with the last 119 points of the training data, and the test data is padded with the last 119 points of the validation data."
      ],
      "metadata": {
        "id": "q7oGXlnHmucr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "datamodule = DataModule(data = data,\n",
        "                        time_name = 't', input_names = ['X'], output_names = ['y'],\n",
        "                        time_unit = 's',\n",
        "                        transforms = {'all': FeatureTransform('identity')},\n",
        "                        pct_train_val_test = [0.7, 0.15, 0.15],\n",
        "                        input_len = [120], output_len = [1],\n",
        "                        batch_size = -1,\n",
        "                        pad_data = True,\n",
        "                        shift = [0], stride = 1)\n"
      ],
      "metadata": {
        "id": "L566g-gdnvKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# lightingmodule\n",
        "loss_fn = Loss('mse', dims = (0, 1))\n",
        "\n",
        "opt = torch.optim.Adam(params = model.parameters(), betas = [0.9, 0.999], lr = 0.002)\n",
        "\n",
        "seq_module = SequenceModule(model = model,\n",
        "                            opt = opt, loss_fn = loss_fn,\n",
        "                            track = True)\n"
      ],
      "metadata": {
        "id": "-5H4k_0-nvK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [pl.callbacks.EarlyStopping(monitor = 'val_epoch_loss',\n",
        "                                        patience = 20,\n",
        "                                        verbose = False,\n",
        "                                        min_delta = 0,\n",
        "                                        mode = 'min')]\n",
        "\n",
        "seq_module.fit(datamodule = datamodule,\n",
        "               max_epochs = 300,\n",
        "               callbacks = callbacks)"
      ],
      "metadata": {
        "id": "CzHkXWjUnvK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_module.predict()"
      ],
      "metadata": {
        "id": "hVrVzw99q9y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_module.plot_predictions()"
      ],
      "metadata": {
        "id": "VfwaQhRjq9zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that the predictive performance of this CNN-based model is near perfect, again as expected."
      ],
      "metadata": {
        "id": "L5Zw-itVrFOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_module.plot_history(['mse_y'], plot_train_history_by = 'epochs' )"
      ],
      "metadata": {
        "id": "JclXMb4-q9zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "impulse_response = seq_module.model.generate_impulse_response(120)\n",
        "\n",
        "plt.plot(h, 'k', label = 'h')\n",
        "plt.plot(impulse_response[0][0], '--r', label = 'Model-based')\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "h0xiJ14rq9zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, our predicted impulse response is near perfect. Note that since our model is CNN-based, the kernel itself should have the form of `h`. Let's check by plotting the kernel."
      ],
      "metadata": {
        "id": "5GVL0wX78zyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kernel = seq_module.model.seq_base[0].base.weight[0].detach().squeeze().flip(0)\n",
        "\n",
        "plt.plot(h, 'k', label = 'h')\n",
        "plt.plot(kernel, '--r', label = 'CNN kernel')\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "BmTkBH5z9fgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed, we get back the original impulse resonse."
      ],
      "metadata": {
        "id": "ruc8OLxB9tsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This walkthrough demonstrates how to use the `TorchTimeSeries` package. Please take a look at more [examples](https://github.com/bchenley/TorchTimeSeries/tree/main/examples) to see how the TorchTimeSeries package can be used to model more challenging cases."
      ],
      "metadata": {
        "id": "jOfQfISN9xTw"
      }
    }
  ]
}