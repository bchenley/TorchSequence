{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3d+Sv8SxWvNZMddkiW+Bd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bchenley/TorchTimeSeries/blob/main/Baroreflex/notebooks/SubjectDataCollection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this notebook is to create a dataset of subjects from the publicly available PhysioNet [website](https://physionet.org/content/autonomic-aging-cardiovascular/1.0.0/) for the user should they wish to collect a different set other than the one provided in the [Baroreflex](https://github.com/bchenley/Baroreflex) repository. If you'd like to use the data already available in the repo (three subjects), it is available [here](https://github.com/bchenley/Baroreflex/blob/main/data/cv_data.pkl)."
      ],
      "metadata": {
        "id": "EbEu22HxilrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create your own dataset, you'll need to have the file containing the .hea data already downloaded on your computer."
      ],
      "metadata": {
        "id": "zY6_oENVjcww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To begin, clone the Baroreflex repo."
      ],
      "metadata": {
        "id": "e6oHaKd0jzGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/bchenley/Baroreflex.git"
      ],
      "metadata": {
        "id": "GuYME-Ged7Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, install the WFDB library."
      ],
      "metadata": {
        "id": "y_XdUe8Tj551"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wfdb"
      ],
      "metadata": {
        "id": "5YDBrt9Leo3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary libraries."
      ],
      "metadata": {
        "id": "WylZjSfVkAXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wfdb, glob, random, os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "metadata": {
        "id": "jWA84ZJ3fRmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have the file containing the data on your Google Drive, then mount your Drive:"
      ],
      "metadata": {
        "id": "4xq9iTOjkExS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "efI9nWVfeyyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The script below will loop through 10 randomly selected subjects in the file. You may increase (up to 1121) or decrease the number of subjects, however be aware of your memory. Subjects with data found to be too corrupt for preprocessing are ignored. However, there may be more, so visualize the data to check. To save space, only six minutes of data is kept for each subject. Again, you may increase or dicrease as desired (range 8 to 45 minutes across all subjects)."
      ],
      "metadata": {
        "id": "Ic2dVw9BkXyg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl2ynPcFd3RM"
      },
      "outputs": [],
      "source": [
        "# Load the .hea file (Replace with the path to your .hea file)\n",
        "record_path = '/content/drive/MyDrive/BIG_IDEAs_Lab/autonomic-aging-a-dataset-to-quantify-changes-of-cardiovascular-autonomic-function-during-healthy-aging-1.0.0/'\n",
        "\n",
        "hea_files = glob.glob(record_path + '/*.hea')\n",
        "subject_info_csv = glob.glob(record_path + '/*.csv')[0]\n",
        "subject_info_df = pd.read_csv(subject_info_csv)\n",
        "\n",
        "num_subjects = len(hea_files)\n",
        "num_subjects_used = 10\n",
        "\n",
        "random.shuffle(hea_files)\n",
        "\n",
        "# some data you should ignore because the recordings are bad\n",
        "ignore_ids = [963, 587, 633, 554, 793, 96, 41, 16, 653, 209, 31, 1060, 332, 936,\n",
        "              559, 140, 186, 584, 365]\n",
        "\n",
        "dt = 1/1000\n",
        "\n",
        "max_minute = 6.0\n",
        "offset = 30/60\n",
        "\n",
        "cv_data = [] # dictionary containing the data from all the selected subjects\n",
        "n = -1\n",
        "\n",
        "while len(cv_data) < num_subjects_used:\n",
        "  n += 1\n",
        "\n",
        "  file_path = hea_files[n]\n",
        "\n",
        "  if file_path.endswith('.hea'): file_path = file_path[:-4]\n",
        "\n",
        "  id_n = int(os.path.splitext(file_path)[0].split('/')[-1])\n",
        "\n",
        "  if os.path.exists(file_path + '.dat') & (id_n not in ignore_ids):\n",
        "\n",
        "    info_n = subject_info_df[subject_info_df['ID'] == id_n]\n",
        "\n",
        "    record = wfdb.rdrecord(file_path)\n",
        "    signal = record.p_signal\n",
        "\n",
        "    dict_n = {'id': id_n}\n",
        "    t = np.arange(signal.shape[0])*dt\n",
        "    signal = signal[(t/60 > offset) & (t/60 < (max_minute+offset)), :]\n",
        "    t = t[(t/60 > offset) & (t/60 < (max_minute+offset))]\n",
        "\n",
        "    dict_n['ecg'], dict_n['abp'] = signal[:, 0], signal[:, -1]\n",
        "    dict_n['t'] = t\n",
        "\n",
        "    if (np.sum(dict_n['abp'] < 50)/dict_n['abp'].shape[0] > 0.05) \\\n",
        "       | (np.sum(dict_n['abp'] > 200)/dict_n['abp'].shape[0] > 0.05):\n",
        "      print(f\"Subject {id_n} has bad abp\")\n",
        "    else:\n",
        "      dict_n['age'] = int(info_n['Age_group']) if pd.notna(info_n['Age_group'].item()) else np.nan\n",
        "      dict_n['sex'] = bool(int(info_n['Sex'])) if pd.notna(info_n['Sex'].item()) else np.nan\n",
        "      dict_n['length'] = info_n['Length'] if pd.notna(info_n['Length'].item()) else np.nan\n",
        "      dict_n['device'] = info_n['Device'] if pd.notna(info_n['Device'].item()) else np.nan\n",
        "      cv_data.append(dict_n)\n",
        "\n",
        "  else:\n",
        "    print(f\"Subject {id_n} does not have a .dat file.\")\n",
        "\n",
        "  print(f\"Subject {id_n} ({len(cv_data)}/{num_subjects_used})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may save your data dictionary in your desired location."
      ],
      "metadata": {
        "id": "VUhEjVypmLcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/cv_data.pkl\"\n",
        "with open(file_path, \"wb\") as file:\n",
        "  pickle.dump(cv_data, file)"
      ],
      "metadata": {
        "id": "myb0iew1iIzt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}